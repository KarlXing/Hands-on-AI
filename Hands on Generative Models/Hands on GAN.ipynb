{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN is a deep neural network archiecture which comprises of a generator (G) and a discriminator (D). Both G and D are neural networks. The generator learns to generate samples as close as possible to the real data while the discriminator learns to distinguish the generated samples from real data. That's why it called to be adversarial. Unlike in VAE, we don't have an explicit density function in this two-player game. So GAN is mainly used to generate realistic samples while VAE is also used to extract a representation of the input. The architecture of GAN is like below:\n",
    "\n",
    "<img src=\"src/gan.png\" width=\"50%\" height=\"50%\" />\n",
    "\n",
    "The generator will output a data instance while the discriminator will output a value in (0, 1) which represents the likelihood that the input is real data. So the discriminator wants to maximize the output for the real data and minimize the output for the generated data while the generator wants to maximize the discriminator's output for the generated data. As a result, the optimization is a minimax game\n",
    "\n",
    "$$\n",
    "\\min\\limits_{\\theta_g}\\max\\limits_{\\theta_d} \\big[\\mathbb{E}_{x\\sim p_{data}}\\log D_{\\theta_d}(x) + \\mathbb{E}_{z\\sim p(z)} \\log(1 - D_{\\theta_d}(G_{\\theta_g}(z)))\\big]\n",
    "$$\n",
    "\n",
    "Okay! Let's move on to the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import random\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters and dataset\n",
    "args = edict()\n",
    "args.batch_size = 64\n",
    "args.epochs = 25\n",
    "args.lr = 0.0002\n",
    "args.nz = 100  # size of input vector to generator\n",
    "args.ngf = 64  # base channels used in generator\n",
    "args.ndf = 64  # base channels used in discriminator\n",
    "args.input_size = 64  # resize input image \n",
    "args.real_label = 1\n",
    "args.fake_label = 0\n",
    "args.beta1 = 0.5  # for optimizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data/MNIST', download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize(args.input_size),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5,), (0.5,)),\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight initialization for generator and discriminator from dcgan paper\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generator and discriminator \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, 1, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # 1 x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # 1 x 64 x 64\n",
    "            nn.Conv2d(1, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #  (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # 1 x 1 x 1\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration in generator and discriminator are based on DCGAN paper. The suggestions from the paper include:\n",
    "1. Replace all max pooling with convolutional stride\n",
    "2. Use transposed convolution for upsampling.\n",
    "3. Eliminate fully connected layers.\n",
    "4. Use Batch normalization except the output layer for the generator and the input layer of the discriminator.\n",
    "5. Use ReLU in the generator except for the output which uses tanh.\n",
    "6. Use LeakyReLU in the discriminator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generator, discriminator, criterion, optimizer\n",
    "netG = Generator(args.nz, args.ngf).to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "netD = Discriminator(args.ndf).to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Training\n",
    "\n",
    "# generate an image based on fixed_noise to check the progress of training\n",
    "fixed_noise = torch.randn(args.batch_size, args.nz, 1, 1, device=device)  \n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    print(\"now training %dth epoch\" % epoch)\n",
    "    for i, data in enumerate(data_loader):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_input = data[0].to(device)\n",
    "        batch_size = real_input.size(0)\n",
    "        label = torch.full((batch_size,), args.real_label, device=device)\n",
    "\n",
    "        output = netD(real_input)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, args.nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(args.fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(args.real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "        \n",
    "    # generate image to check progress\n",
    "    fake = netG(fixed_noise)\n",
    "    save_image(fake.detach(), 'GAN_result/sample_%03d.png' % (epoch), normalize=True)\n",
    "\n",
    "# save model after training\n",
    "torch.save(netG.state_dict(), 'GAN_result/netG_epoch.pth')\n",
    "torch.save(netD.state_dict(), 'GAN_result/netD_epoch.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting thing is that, if you train the model for more epochs, you may find that the generated image become worse. This is because GAN is not stable. You may add noise to the input of discriminator to mitigate this problem. How to make GAN more stable is still a hot topic. You could refer to more GAN variants to get more information! Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
