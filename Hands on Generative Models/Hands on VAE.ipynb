{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested by the title, VAE is extended from autoencoders. In autoencoders, we have an encoder which maps the input $x$ to a latent variable $z$ and a decoder then reconstruct $\\hat{x}$ from $z$. The learned feature $z$ could serve as a representation of the input since most important information is already included in $z$ if $\\hat{x}$ is close to $x$.\n",
    "\n",
    "<img src=\"src/autoencoder.png\" width=\"30%\" height=\"30%\" />\n",
    "\n",
    "In VAE, we add probability to the autoencoder and define the density function of $x$ based on $z$ in the form of:  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p_\\theta(x) = \\int p_\\theta(z)p_\\theta(x|z)dz\n",
    "\\end{equation}\n",
    "$$ \n",
    "In practice, we choose a simple distribution like Gaussian for $p(z)$ and use a neural network to represent the complex conditional distribution $p(x|z)$. We learn the parameters $\\theta$ by maximizing the likelihood of the training data. \n",
    "\n",
    "Although the integration over $z$ is intractable, we could optimize a lower bound of $p(x)$ with the help of the encoder and bayes rule. The encoder provides a conditional probability distribution $q_\\phi (z|x)$. As a result, the logarithm of the likelihood of training data could be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "log p_\\theta(x^{i}) &=  E_{z\\sim q_\\phi(z|x^{i})} \\big[log \\frac{p_\\theta(z)\\ p_\\theta(x^{i}|z)}{p_\\theta(z|x^i))} \\frac{q_\\phi(z|x^{i})}{q_\\phi(z|x^i}\\big]  \\\\\n",
    " &= E_z\\big[log p_\\theta(x^i|z)\\big] - E_z\\big[\\frac{q_\\phi(z|x^i)}{p_\\theta(z)}\\big] + E_z\\big[\\frac{q_\\phi(z|x^i)}{p_\\theta(z|x^i)}\\big] \\\\\n",
    " &= E_z\\big[logp_\\theta(x^i|z)\\big] - D_{KL}(q_\\phi(z|x^i) \\| p_\\theta(z)) + D_{KL}(q_\\phi(z|x^i) \\|p_\\theta(z|x^i) )\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Although the third term $D_{KL}(q_\\phi(z|x^i) \\|p_\\theta(z|x^i)$ in equation $4$ is still intractable, it's always bigger than 0. We ignore it and optimize the first two terms which is a lower bound of $log p_\\theta(x^i)$. \n",
    "\n",
    "The maximization of $logp_\\theta(x^i)$ could be factorized into maximization of $E_z\\big[logp_\\theta(x^i|z)\\big]$ (reconstruct input as best as possible) and minimization of $D_{KL}(q_\\phi(z|x^i) \\| p_\\theta(z))$ (make approximate posteior probability distribution close to prior ) \n",
    "\n",
    "Then the architecture of VAE is like below:\n",
    "\n",
    "<img src=\"src/vae.png\" width=\"30%\" height=\"30%\" />\n",
    "\n",
    "As we can see, the output of the encoder and decoder are nolonger deterministic $z$ or $x^i$ but are probability distributions! Based on probability distributions, we sample $z$ and $x^i$. Besides that, with reparameterization trick, we make sampling also differentiable. Okay! Let's get through a project of VAE on MNIST to better understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize parameters and dataset\n",
    "args = edict()\n",
    "args.batch_size = 128\n",
    "args.epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data/MNIST', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data/MNIST', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.en1 = nn.Linear(784, 400)\n",
    "        self.en2_mu = nn.Linear(400, 20)\n",
    "        self.en2_logvar = nn.Linear(400, 20)\n",
    "        self.de1 = nn.Linear(20, 400)\n",
    "        self.de2 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.en1(x))\n",
    "        return self.en2_mu(h1), self.en2_logvar(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.de1(z))\n",
    "        return torch.sigmoid(self.de2(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assume the prior of $z$ is a Gaussian distribution and the conditional probability distribution of the decoder is a bernoulli distribution. That's why the encoder has two outputs and the decoder has only one. Reparameterize function simulates the sampling based on $\\mu + randn * std$ that makes $z$ also differentiable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As introduced above, there are two terms for optimization which are BCE and KLD in the loss function defined above. Actually the bernoulli distribution might be a little inappropriate here because the inputs in MNIST daataset are not binary. However, since most values in the input are 0 or 1, we still think of them as approximately binary. Then the binary cross entropy between the inputs and the reconstructed bernoulli probability $y$ is indeed the first term in equation $4$. \n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "logp(x|z) &= log(x^y(1-x)^{1-y})  \\\\\n",
    "&= ylog(x) + (1-y)log(1-x) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then let's look into the second term in equation $4$ which is the KL divergence between $q(z|x)$ and $p(z)$. Since both of them are Gaussian distributions, we assume $q \\sim \\mathcal{N}(\\mu_1, \\sigma_1)$ and $p \\sim \\mathcal{N}(\\mu_2,\\sigma_2)$. Then it could be written as (The derivation between two gaussian distributions are inducted later, here we just show the conclusion):\n",
    "$$\n",
    "\\begin{align}D_{KL}(q_\\phi(z|x)\\|p(z)) & = -\\frac{1}{2} + log(\\frac{\\sigma_2}{\\sigma_1}) + \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2}  \\\\&= -\\frac{1}{2}(1 + log\\sigma_1 - \\sigma_1^2-\\mu_1^2) \\qquad where \\ \\mu_2 = 0, \\sigma_2 = 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then, we can see that they match exactly with BCE and KLD in the loss function. The derivation for KL divergence between two gaussian distributions is shown below. You may skip it if you just want to know the basic idea of VAE. \n",
    "\n",
    "$$\n",
    "p \\sim \\mathcal{N}(\\mu_1, \\sigma_1), q \\sim \\mathcal{N}(\\mu_2, \\sigma_2) \\\\\\begin{align}D_{KL}(p\\|q) & = -\\int p(x)log(q(x))dx + \\int p(x) log(p(x))dx  \\\\&= -\\int p(x) log\\frac{1}{(2\\pi\\sigma_2^2)^{1/2}}e^{-\\frac{(x-\\mu_2)^2}{2\\sigma_2^2}}dx + \\int p(x)log\\frac{1}{(2\\pi\\sigma_1^2)^{1/2}}e^{-\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}}dx  \\\\&= \\frac{1}{2}log(2\\pi\\sigma_2^2) + \\int p(x)\\frac{(x-\\mu_2)^2}{2\\sigma_2^2}dx - \\frac{1}{2} log(2\\pi\\sigma_1^2) -\\int p(x)\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}dx \\\\&= log\\frac{\\sigma_2}{\\sigma_1} + \\frac{1}{2\\sigma_2^2} \\big[\\int p(x)x^2dx - \\int p(x)2x\\mu_2dx + \\int p(x)\\mu_2^2dx \\big] - \\frac{1}{2}  \\\\&= log\\frac{\\sigma_2}{\\sigma_1}  + \\frac{\\sigma_1^2 + \\mu_1^2 -2\\mu_1\\mu_2 + \\mu_2^2}{2\\sigma_2^2} - \\frac{1}{2}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we create the model and optimizer\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test function\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'VAE_result/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the trainning and see results\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(64, 20).to(device)\n",
    "        sample = model.decode(sample).cpu()\n",
    "        save_image(sample.view(64, 1, 28, 28),\n",
    "                   'VAE_result/sample_' + str(epoch) + '.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can see the reconstruction and sampling results in VAE_result directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
